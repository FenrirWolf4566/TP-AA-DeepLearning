{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "110b3ea8",
   "metadata": {},
   "source": [
    "<font  style=\"font-size: 4rem; color: darkviolet\"> Transfer Learning with MobileNetV2 </font>\n",
    "\n",
    "AA - 2022/23 - TP8\n",
    "\n",
    "*This assignement is inspired by the Deep Learning course on Coursera by Andrew Ng, Stanford University, for which we are thankful.*\n",
    "\n",
    "In this assignment, you'll be using transfer learning on a pre-trained CNN to build an Alpaca/Not Alpaca classifier.\n",
    "\n",
    "A pre-trained model is a network that's already been trained on a large dataset and saved, which allows you to use it to customize your own model cheaply and efficiently. The network you'll be using, MobileNetV2, was designed to provide fast and computationally efficient performance. It's been pre-trained on ImageNet, a dataset containing over 14 million images and 1000 classes.\n",
    "\n",
    "For more insights, please refer to the lecture material.\n",
    "\n",
    "Objectives:\n",
    "\n",
    "- Create a dataset from a directory\n",
    "- Preprocess and augment data using the Sequential API\n",
    "- Adapt the pre-trained MobileNet model to the new data using the Functional API \n",
    "- Fine-tune the final layers of the classifier to further improve the accuracy of the model\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "- [1 - The Dataset](#1)\n",
    "- [2 - Pre-process and Augment Training Data](#2) \n",
    "    - [Exercise 2.1 - data_augmenter](#ex-2.1)\n",
    "- [3 - Using MobileNetV2 for Transfer Learning](#3)\n",
    "    - [3.1 - The MobileNetV2 architecture](#3-1)\n",
    "    - [3.2 - Layer Freezing with the Functional API](#3-2)\n",
    "        - [Exercise 3.2.1 - alpaca_model](#ex-3.2.1)\n",
    "    - [3.3 - Fine-tuning the Model](#3.3)\n",
    "        - [Exercise 3.3.1](#ex-3.3.1)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de70dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfl\n",
    "\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc9235a",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "# <font color='darkviolet'> 1 - The Dataset\n",
    "       \n",
    "When it comes to training and evaluating deep learning models in Keras, generating a dataset from image files stored on disk is usually a straightforward and efficient process. Use `image_dataset_from_directory()` to read data from the directory containing the image files, and create both the training and validation datasets.\n",
    "\n",
    "If you wish to specify a validation split, you'll need to set the `subset` for each portion. Assign the value `subset='training'` to the training set and `subset='validation'` to the validation set.\n",
    "\n",
    "Additionally, it's important to ensure that your training and validation sets do not overlap. To achieve this, make sure to set your seeds to match each other. This will help ensure that the random sampling for each set is consistent and reproducible, resulting in more reliable model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1efa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (160, 160)\n",
    "directory = \"data/dataset/\"\n",
    "# Use a random seed of 42 to ensure the same splits each time the code is run.\n",
    "train_dataset = image_dataset_from_directory(directory,\n",
    "                                             shuffle=True,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             image_size=IMG_SIZE,\n",
    "                                             validation_split=0.2,\n",
    "                                             subset='training',\n",
    "                                             seed=42)\n",
    "\n",
    "validation_dataset = image_dataset_from_directory(directory,\n",
    "                                             shuffle=True,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             image_size=IMG_SIZE,\n",
    "                                             validation_split=0.2,\n",
    "                                             subset='validation',\n",
    "                                             seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d897fce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset.element_spec)\n",
    "print(validation_dataset.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fa4b2f",
   "metadata": {},
   "source": [
    "**Note:** The original dataset has some mislabelled images.\n",
    "\n",
    "Let's take a look at some of the images from the training set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa87ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_dataset.class_names\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "for images, labels in train_dataset.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abe0513",
   "metadata": {},
   "source": [
    "#### <font color='blue'> Q1.1 Describe your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e46a985",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "# <font color='darkviolet'> 2 - Preprocess and Augment Training Data   \n",
    "    \n",
    "Using `prefetch()` is an effective way to prevent a potential memory bottleneck that can occur when reading from disk. This method creates a source dataset from your input data and applies a transformation to preprocess it. By iterating over the dataset one element at a time, the `prefetch()` method sets aside some data and keeps it ready for when it's needed. Because the iteration is streaming, the data doesn't need to fit into memory.\n",
    "\n",
    "You have the option to manually set the number of elements to prefetch, or you can use `tf.data.experimental.AUTOTUNE` to automatically choose the parameters. By using AUTOTUNE, TensorFlow automatically tunes the prefetch value at runtime. \n",
    "\n",
    "To increase diversity in the training set and help your model better learn the data, it's common practice to **augment the images** by transforming them. This can include randomly flipping and rotating them. With Keras' Sequential API, data augmentation is straightforward, with built-in and customizable preprocessing layers. These layers are saved with the rest of your model and can be re-used later.\n",
    "\n",
    "For further information on data augmentation, please refer to the official Keras documentation https://www.tensorflow.org/tutorials/images/data_augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a67e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8837b35",
   "metadata": {},
   "source": [
    "<a name='ex-2.1'></a>\n",
    "### <font color='blue'> Exercise 2.1 - data_augmenter\n",
    "\n",
    "Implement a function for data augmentation. Use a `Sequential` keras model composed of 2 layers:\n",
    "* `RandomFlip('horizontal')`\n",
    "* `RandomRotation(0.2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d548a6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmenter():\n",
    "    '''\n",
    "    Creates a Sequential model composed of 2 layers for data augmentation.\n",
    "    Returns:\n",
    "        tf.keras.Sequential: a model for data augmentation.\n",
    "    '''\n",
    "    # Create an empty Sequential model for data augmentation\n",
    "    # TODO\n",
    "    #data_augmentation = ...\n",
    "    \n",
    "    # Add a RandomFlip layer to randomly flip images horizontally\n",
    "    # TODO\n",
    "    #data_augmentation.add(...)\n",
    "    \n",
    "    # Add a RandomRotation layer to randomly rotate images by a facotr 0.2\n",
    "    # TODO\n",
    "    #data_augmentation.add(...)\n",
    "    \n",
    "    return data_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaaadbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmenter = data_augmenter()\n",
    "\n",
    "assert(augmenter.layers[0].name.startswith('random_flip')), \"First layer must be RandomFlip\"\n",
    "assert augmenter.layers[0].mode == 'horizontal', \"RadomFlip parameter must be horizontal\"\n",
    "assert(augmenter.layers[1].name.startswith('random_rotation')), \"Second layer must be RandomRotation\"\n",
    "assert augmenter.layers[1].factor == 0.2, \"Rotation factor must be 0.2\"\n",
    "assert len(augmenter.layers) == 2, \"The model must have only 2 layers\"\n",
    "\n",
    "print('\\033[92mAll tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79ec626",
   "metadata": {},
   "source": [
    "Take a look at how a single image from the training set can be transformed into nine different variations with just a few lines of code. \n",
    "\n",
    "By applying simple transformations such as flipping and rotating, we can create a diverse set of images for the model to learn from. These data augmentations can help improve the model's ability to generalize and make accurate predictions on new, unseen images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4875e20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data augmentation model\n",
    "data_augmentation = data_augmenter()\n",
    "\n",
    "# Take one image from the training dataset\n",
    "for image, _ in train_dataset.take(1):\n",
    "    \n",
    "    # Create a plot with a 3x3 grid of images\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    \n",
    "    # Select the first image from the batch\n",
    "    first_image = image[0]\n",
    "    \n",
    "    # Apply data augmentation to the first image and plot the result\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        \n",
    "        # Apply data augmentation to the image\n",
    "        augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n",
    "        \n",
    "        # Display the augmented image\n",
    "        plt.imshow(augmented_image[0] / 255)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c164ebc",
   "metadata": {},
   "source": [
    "Next, you'll apply a useful tool from the MobileNet application in TensorFlow to normalize your input. Since the pre-trained MobileNetV2 model was originally trained using normalization values in the range of [-1, 1], it's considered best practice to use the same normalization standard for your input data. You can achieve this easily by using the `tf.keras.applications.mobilenet_v2.preprocess_input` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b999f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4d177b",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## <font color='darkviolet'> 3 - Using MobileNetV2 for Transfer Learning\n",
    "\n",
    "MobileNetV2 is a convolutional neural network architecture that was originally trained on the ImageNet dataset. It was specifically designed to provide fast and computationally efficient performance on mobile and other low-compute devices. The network consists of 155 layers and is highly effective for object detection and image segmentation tasks, as well as classification tasks like the one you're working on.\n",
    "\n",
    "<a name='3-1'></a>\n",
    "### <font color='darkviolet'> 3.1 - The MobileNetV2 architecture\n",
    "    \n",
    "MobileNetV2's architecture has three defining characteristics (as seen in last lecture):\n",
    "\n",
    "*   Depthwise separable conv layers\n",
    "*   Expansion layers\n",
    "*   Shortcut connections between bottleneck blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31641a0",
   "metadata": {},
   "source": [
    "Let's train your base model using all the layers from the pre-trained model. \n",
    "\n",
    "Similarly to how you re-used the pre-trained normalization values MobileNetV2 was trained on, you'll load the pre-trained weights from ImageNet by specifying `weights='imagenet'`. \n",
    "\n",
    "**Note** the last layer in this model is the so called top layer and it is responsible of the classification in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13240f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = IMG_SIZE + (3,) #for the RGB color channels\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=True,\n",
    "                                               weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eb9286",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c803c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_layers = len(base_model.layers)\n",
    "print(base_model.layers[nb_layers - 2].name)\n",
    "print(base_model.layers[nb_layers - 1].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28d8169",
   "metadata": {},
   "source": [
    "Notice some of the layers in the summary like `Conv2D` and `DepthwiseConv2D` and how they follow the progression of expansion to depthwise convolution to projection. In combination with BatchNormalization and ReLU, these make up the bottleneck blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cf3e5a",
   "metadata": {},
   "source": [
    "####  <font color='blue'> Q3.1.1: What is the total number of blocks present in the MobileNetV2 architecture?\n",
    "\n",
    "#### <font color='blue'> Q3.1.2: In the MobileNetV2 architecture, what is the role of the expansion layer and how does it contribute to the overall model performance?\n",
    "        \n",
    "#### <font color='blue'> Q3.1.3: What is the significance of the depthwise separable convolution layer in the MobileNetV2 architecture?\n",
    "    \n",
    "#### <font color='blue'> Q3.1.4: How is the skip connection used in the MobileNetV2 architecture? Can you locate it in the model summary?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c91d46b",
   "metadata": {},
   "source": [
    "Next, choose the first batch from the dataset and run it through the MobileNetV2 base model to test out the predictions on some of your images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf0327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(iter(train_dataset))\n",
    "feature_batch = base_model(image_batch)\n",
    "print(feature_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353a3495",
   "metadata": {},
   "source": [
    "#### <font color='blue'> Q3.1.5 What do the numbers 32 and 1000 refer to ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a849687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows the label probabilities in one tensor \n",
    "label_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c840aa",
   "metadata": {},
   "source": [
    "The predictions returned by the base model below follow this format: first the class number, then a human-readable label, and last the probability of the image belonging to that class. There are two of these returned for each image in the batch - the top two probabilities returned for that image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f2f95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False\n",
    "image_var = tf.Variable(preprocess_input(image_batch))\n",
    "pred = base_model(image_var)\n",
    "\n",
    "tf.keras.applications.mobilenet_v2.decode_predictions(pred.numpy(), top=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e004a383",
   "metadata": {},
   "source": [
    "You may notice that none of the labels say \"alpaca\". This is because the MobileNet was pre-trained over ImageNet tht doesn't have the correct classification labels for alpacas.\n",
    "\n",
    "You can delete the top layer, which contains all the classification labels, and create a new classification layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba878913",
   "metadata": {},
   "source": [
    "<a name='3-2'></a>\n",
    "## <font color='darkviolet'> 3.2 - Layer Freezing with the Functional API\n",
    "\n",
    "In the next sections, you'll see how you can use a pre-trained model to modify the classifier task so that it's able to recognize alpacas. You can achieve this in three steps: \n",
    "\n",
    "1. Delete the top layer (the classification layer)\n",
    "    * Set `include_top` in `base_model` as False\n",
    "2. Add a new classification layer\n",
    "    * Train only one layer by freezing the rest of the network\n",
    "    * A single neuron is enough to solve a binary classification problem\n",
    "3. Freeze the base model and train the newly-created classification layer\n",
    "    * Set `base model.trainable=False` to avoid changing the weights and train *only* the new layer\n",
    "    * Set training in `base_model` to False to avoid keeping track of statistics in the batch norm layer ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53798d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SIZE + (3,),\n",
    "                                                   include_top=False,\n",
    "                                                   weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1e7ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780fa06a",
   "metadata": {},
   "source": [
    "<a name='ex-3.2.1'></a>\n",
    "### <font color='blue'> 3.2.1 Exercise - alpaca_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c9192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.test_utils import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9c7088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpaca_model(image_shape=IMG_SIZE, data_augmentation=data_augmenter(), dropout_rate=0.2):\n",
    "    \"\"\"\n",
    "    Define a binary classification model based on MobileNetV2.\n",
    "    \n",
    "    Args:\n",
    "    image_shape -- Image width and height\n",
    "    data_augmentation -- Data augmentation function\n",
    "    dropout_rate -- Dropout rate\n",
    "    \n",
    "    Returns:\n",
    "    A Keras model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input shape\n",
    "    input_shape = image_shape + (3,)\n",
    "    \n",
    "    # Load the pre-trained MobileNetV2 model\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=input_shape,\n",
    "                                                   include_top=False,\n",
    "                                                   weights='imagenet')\n",
    "    \n",
    "    # Freeze the base model by making it non trainable\n",
    "    #TODO\n",
    "    #base_model.trainable = ...\n",
    "\n",
    "    # Create the input layer - same as the imageNetv2 input size\n",
    "    #TODO\n",
    "    #inputs = tf.keras.Input(...) \n",
    "    \n",
    "    # Apply data augmentation to the inputs\n",
    "    #TODO\n",
    "    #x = ... if data_augmentation else ...\n",
    "    \n",
    "    # Preprocess the inputs\n",
    "    #TODO\n",
    "    #x = ...\n",
    "    \n",
    "    # Pass the inputs through the base model\n",
    "    #TODO\n",
    "    #base_model_output = ... \n",
    "    \n",
    "    # Add new binary classification layers\n",
    "    # Use global avg pooling to summarize the info in each channel\n",
    "    x = tfl.GlobalAveragePooling2D()(base_model_output) \n",
    "    # Include dropout to avoid overfitting\n",
    "    #TODO\n",
    "    #x = tfl.Dropout(...)(...)\n",
    "    # Create a prediction layer with one neuron\n",
    "    #TODO\n",
    "    #prediction_layer = ...\n",
    "    \n",
    "    # Connect the layers to define the model\n",
    "    outputs = prediction_layer(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e777459",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = alpaca_model(IMG_SIZE, data_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302c3c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02751813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.test_utils import summary, comparator\n",
    "\n",
    "alpaca_summary = [['InputLayer', [(None, 160, 160, 3)], 0],\n",
    " ['Sequential', (None, 160, 160, 3), 0],\n",
    " ['TFOpLambda', (None, 160, 160, 3), 0],\n",
    " ['TFOpLambda', (None, 160, 160, 3), 0],\n",
    "#['TensorFlowOpLayer', [(None, 160, 160, 3)], 0],\n",
    "#['TensorFlowOpLayer', [(None, 160, 160, 3)], 0],\n",
    " ['Functional', (None, 5, 5, 1280), 2257984],\n",
    " ['GlobalAveragePooling2D', (None, 1280), 0],\n",
    " ['Dropout', (None, 1280), 0, 0.2],\n",
    " ['Dense', (None, 1), 1281, 'linear']] #linear is the default activation\n",
    "    \n",
    "comparator(summary(model2), alpaca_summary)\n",
    "\n",
    "for layer in summary(model2):\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8572c47",
   "metadata": {},
   "source": [
    "The base learning rate has been set for you, so you can go ahead and compile the new model and run it for 5 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1551bbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learning_rate = 0.001\n",
    "model2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab30526",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epochs = 5\n",
    "history = model2.fit(train_dataset, validation_data=validation_dataset, epochs=initial_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3169f6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = [0.] + history.history['accuracy']\n",
    "val_acc = [0.] + history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519a4e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7dfb08",
   "metadata": {},
   "source": [
    "The results are ok, but could be better. Next, try some fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b396aec5",
   "metadata": {},
   "source": [
    "<a name='3.3'></a>\n",
    "### <font color='darkviolet'>  3.3 - Fine-tuning the Model\n",
    "\n",
    "Fine-tune the model by re-running the optimizer in the last layers to improve accuracy. When you use a smaller learning rate, you take smaller steps to adapt it a little more closely to the new data. The way you achieve this is by unfreezing the layers at the end of the network, and then re-training your model on the final layers with a very low learning rate. \n",
    "\n",
    "Where the final layers actually begin is a a bit arbitrary, so feel free to play around with this number. The important point is that the later layers are the part of your network that contain the fine details (pointy ears, hairy tails) that are more specific to your current task.\n",
    "\n",
    "First, unfreeze the base model by setting `base_model.trainable=True`, set a layer to fine-tune from, then re-freeze all the layers before it. Run it again for another few epochs, and see if your accuracy improved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fda95a7",
   "metadata": {},
   "source": [
    "<a name='#ex-3.3.1'></a>\n",
    "### <font color='blue'> 3.3.1 Exercise - fine_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ea78d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the base model and make it trainable\n",
    "base_model = model2.layers[4]\n",
    "base_model.trainable = True\n",
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = 120\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "# TODO\n",
    "# for layer in base_model.layers[...]:\n",
    "#     layer.... = ...\n",
    "    \n",
    "# Define the loss function, optimizer, and evaluation metric\n",
    "# TODO\n",
    "# loss_function = ...\n",
    "# optimizer = ...\n",
    "# metrics=...\n",
    "\n",
    "# Compile the model\n",
    "# TODO\n",
    "# model2.compile(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87619dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(loss_function) == tf.keras.losses.BinaryCrossentropy, \"Not the correct layer\"\n",
    "assert loss_function.from_logits, \"Use from_logits=True\"\n",
    "assert type(optimizer) == tf.keras.optimizers.Adam, \"This is not an Adam optimizer\"\n",
    "assert optimizer.lr == base_learning_rate / 10, \"Wrong learning rate\"\n",
    "assert metrics[0] == 'accuracy', \"Wrong metric\"\n",
    "\n",
    "print('\\033[92mAll tests passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d9f381",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_epochs = 5\n",
    "total_epochs =  initial_epochs + fine_tune_epochs\n",
    "\n",
    "history_fine = model2.fit(train_dataset,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch=history.epoch[-1],\n",
    "                         validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da94b1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc += history_fine.history['accuracy']\n",
    "val_acc += history_fine.history['val_accuracy']\n",
    "\n",
    "loss += history_fine.history['loss']\n",
    "val_loss += history_fine.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a920fcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "          plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.ylim([0, 1.0])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "         plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c85a3a8",
   "metadata": {},
   "source": [
    "#### <font color='blue'> Q3.3.1 Describe your experiments and results. \n",
    "\n",
    "#### <font color='blue'> Q3.3.2 Describe how you can adapt a pre-trained classifier to new data and and how you can potentially improve its accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
