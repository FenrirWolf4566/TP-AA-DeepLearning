{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "110b3ea8",
   "metadata": {},
   "source": [
    "<font  style=\"font-size: 4rem; color: darkviolet\"> Image Segmentation with U-Net </font>\n",
    "\n",
    "AA - 2022/23 - TP9 \n",
    "\n",
    "*This assignement is inspired by the Deep Learning course on Coursera by Andrew Ng, Stanford University, for which we are thankful.*\n",
    "\n",
    "In this assignment, you'll learn about U-Net, a type of CNN designed for fast and precise image segmentation. Specifically, you'll use it to **predict a label for every pixel** in a self-driving car dataset image, a task known as **semantic image segmentation**.\n",
    "\n",
    "Unlike object detection, which uses bounding boxes to identify objects (including pixels that don't belong to the object), semantic image segmentation allows you to **label each pixel with its corresponding class**, resulting in a precise mask for each object in the image. This is crucial for self-driving cars, which need to accurately identify and understand their surroundings to avoid obstacles and keep people safe.\n",
    "\n",
    "<img src=\"data/images_TP9/carseg.png\" style=\"width:500px;height:250;\">\n",
    "<caption><center> <u><b>Figure 1</u></b>: Example of a segmented image <br> </center></caption>\n",
    "\n",
    "Your objectives for this assignment are to:\n",
    "\n",
    "* Code a U-Net from scratch\n",
    "* Understand the difference between a regular CNN and a U-Net\n",
    "* Use semantic image segmentation on the CARLA self-driving car dataset\n",
    "* Apply sparse categorical cross-entropy for pixel-wise prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e01077",
   "metadata": {},
   "source": [
    "###  <font color='darkviolet'> Table of Content\n",
    "\n",
    "- [1 - The Dataset](#1)\n",
    "- [2 - U-Net](#2)\n",
    "    - [2.1 - Model Details](#2-1)\n",
    "    - [2.2 - Encoder (Downsampling Block)](#2.2)\n",
    "        - [Exercise - conv_block](#ex-2.2.1)\n",
    "    - [2.3 - Decoder (Upsampling Block)](#2.3)\n",
    "        - [Exercise - upsampling_block](#ex-2.3.1)\n",
    "    - [2.4 - Build the Model](#2.4)\n",
    "        - [Exercise - unet_model](#ex-2.4.1)\n",
    "    - [2.5 - Loss Function](#2.5)\n",
    "    - [2.7 - Dataset Handling](#2.7)\n",
    "- [3 - Train the Model](#3)\n",
    "    - [3.1 - Create Predicted Masks](#3.1)\n",
    "- [4 - Conclusion](#4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de70dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dropout \n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import concatenate\n",
    "\n",
    "from data.test_utils import summary, comparator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc9235a",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## <font color='darkviolet'> 1 - The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff082e9",
   "metadata": {},
   "source": [
    "The dataset has subdirectories `CameraRGB` and `CameraMask` containing images and corresponding segmentation masks, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1efa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "path = ''\n",
    "image_path = os.path.join(path, './data/CameraRGB/')\n",
    "mask_path = os.path.join(path, './data/CameraMask/')\n",
    "image_list = os.listdir(image_path)\n",
    "mask_list = os.listdir(mask_path)\n",
    "image_list = [image_path+i for i in image_list] # the file paths for all the images in the dataset\n",
    "mask_list = [mask_path+i for i in mask_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b13fd48",
   "metadata": {},
   "source": [
    "Check out some of the images and corresponding segmentation masks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83175c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=119\n",
    "# Load the image and mask\n",
    "img = imageio.imread(image_list[N])\n",
    "mask = imageio.imread(mask_list[N])\n",
    "\n",
    "# Plot the original image and its corresponding mask side by side\n",
    "fig, arr = plt.subplots(1, 2, figsize=(14, 10))\n",
    "arr[0].imshow(img)\n",
    "arr[0].set_title('Image')\n",
    "arr[1].imshow(mask[:, :, 0])\n",
    "arr[1].set_title('Segmentation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d99680",
   "metadata": {},
   "source": [
    "Revise how to create and process a dataset in TensorFlow: https://www.tensorflow.org/api_docs/python/tf/data/Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52163b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow constants for the image and mask file paths\n",
    "image_filenames = tf.constant(image_list)\n",
    "masks_filenames = tf.constant(mask_list)\n",
    "\n",
    "# Create a TensorFlow dataset from the image and mask constants\n",
    "dataset = tf.data.Dataset.from_tensor_slices((image_filenames, masks_filenames)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dedf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to process image and mask file paths into preprocessed images\n",
    "def process_path(image_path, mask_path):\n",
    "    # Read the image file\n",
    "    img = tf.io.read_file(image_path)\n",
    "    # Decode the image to a 3-channel tensor\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    # Convert the pixel values to the float32 data type\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "\n",
    "    mask = tf.io.read_file(mask_path)\n",
    "    mask = tf.image.decode_png(mask, channels=3)\n",
    "    # Reduce the mask to a tensor with shape (height, width, 1)\n",
    "    mask = tf.math.reduce_max(mask, axis=-1, keepdims=True)\n",
    "    \n",
    "    return img, mask\n",
    "\n",
    "def preprocess(image, mask):\n",
    "    # Resize to the desired dimensions\n",
    "    input_image = tf.image.resize(image, (96, 128), method='bilinear')\n",
    "    input_mask = tf.image.resize(mask, (96, 128), method='nearest')\n",
    "    \n",
    "    return input_image, input_mask\n",
    "\n",
    "\n",
    "image_ds = dataset.map(process_path)\n",
    "processed_image_ds = image_ds.map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abe0513",
   "metadata": {},
   "source": [
    "#### <font color='blue'> Q1.1 Describe your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e46a985",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## <font color='darkviolet'> 2 - U-Net \n",
    "\n",
    "U-Net is a deep learning architecture that was originally designed for detecting tumors in medical images. It gets its name from its distinctive U-shaped design, which enables it to perform accurate image segmentation tasks.\n",
    "\n",
    "At its core, U-Net is a type of Fully Convolutional Network (FCN) that replaces the dense layers of a typical CNN with transposed convolutional layers for upsampling or increasing the resolution of feature maps.\n",
    "\n",
    "U-Net uses a matching number of convolutions to downsample the input image to a feature map, and transposed convolutions to upsample those maps back up to the original input image size. Additionally, it incorporates **skip connections** between corresponding layers in the encoder and decoder networks, allowing the network to retain fine information that would otherwise be lost during encoding. \n",
    "    \n",
    "<a name='2-1'></a>\n",
    "### <font color='darkviolet'> 2.1 - Model Details\n",
    "\n",
    "<img src=\"data/images_TP9/unet.png\" style=\"width:700px;height:400;\">\n",
    "<caption><center> <u><b> Figure 2 </u></b>: U-Net Architecture<br> </center></caption>\n",
    "\n",
    "**Contracting path** (Encoder containing downsampling steps):\n",
    "\n",
    "Images are first fed through several convolutional layers which reduce height and width, while growing the number of channels.\n",
    "\n",
    "The contracting path follows a regular CNN architecture, with convolutional layers, their activations, and pooling layers to downsample the image and extract its features. In detail, it consists of the repeated application of two 3 x 3 convolutions, each followed by a rectified linear unit (ReLU) and a 2 x 2 max pooling operation with stride 2 for downsampling. At each downsampling step, the number of feature channels is doubled.\n",
    "\n",
    "**Expanding path** (Decoder containing upsampling steps):\n",
    "\n",
    "The expanding path performs the opposite operation of the contracting path, growing the image back to its original size, while shrinking the channels gradually.\n",
    "\n",
    "In detail, each step in the expanding path upsamples the feature map, followed by a 2 x 2 convolution (the transposed convolution). This transposed convolution halves the number of feature channels, while growing the height and width of the image.\n",
    "\n",
    "Next is a concatenation with the correspondingly cropped feature map from the contracting path, and two 3 x 3 convolutions, each followed by a ReLU. Cropping removes extra pixels from the feature maps in the contracting path to ensure that the spatial dimensions match those of the expanding path.\n",
    "\n",
    "**Final Feature Mapping Block**: \n",
    "In the final layer, a 1x1 convolutional is used to map each 64-component feature vector to the desired number of classes. The channel dimensions from the previous layer correspond to the number of filters used in that layer, and by using a 1x1 convolution, we can transform that dimension by choosing an appropriate number of 1x1 filters. Using an appropriate number of filters allows us to obtain a new set of feature maps with a reduced number of channels, which can be interpreted as having one layer per class. By reducing the number of channels to one layer per class, we can obtain a probability distribution over the classes for each pixel in the input image, which can be used for image segmentation.\n",
    "\n",
    "**Note**: The figure shows the original U-Net architecture. However, due to computational constraints, you will code a smaller version with half the number of filters. The important thing to remember is that the number of filters is doubled at each step of the architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8837b35",
   "metadata": {},
   "source": [
    "<a name='2.2'></a>\n",
    "## <font color='darkviolet'> 2.2 - Encoder (Downsampling Block) \n",
    "\n",
    "<!-- <img src=\"data/images/encoder.png\" style=\"width:500px;height:500;\">\n",
    "<caption><center> <u><b>Figure 3</u></b>: The U-Net Encoder up close <br> </center></caption> -->\n",
    "\n",
    "The encoder is a stack of conv_blocks:\n",
    "\n",
    "Each `conv_block()` is composed of two **Conv2D** layers  with ReLU activations. We will apply **Dropout**, and **MaxPooling2D** to some conv_blocks, specifically to the last two blocks of the downsampling. \n",
    "\n",
    "The function will  return two tensors: \n",
    "- `next_layer`: that will go into the next block. \n",
    "- `skip_connection`: that will go into the corresponding decoding block.\n",
    "\n",
    "**Note**: If `max_pooling=True`, the `next_layer` will be the output of the MaxPooling2D layer, but the `skip_connection` will be the output of the previously applied layer (Conv2D or Dropout, depending on the case). Else, both results will be identical.  \n",
    "\n",
    "<a name='ex-2.2.1'></a>\n",
    "### <font color='blue'> Exercise 2.2.1 - conv_block\n",
    "\n",
    "Implement `conv_block(...)`. Here are the instructions for each step in the `conv_block`, or contracting block: \n",
    "\n",
    "* Add two **Conv2D** layers with `n_filters` filters with `kernel_size` set to 3, `kernel_initializer` set to ['he_normal'](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/HeNormal), `padding` set to 'same' and 'relu' activation.\n",
    "* if `dropout_prob` > 0, then add a Dropout layer with parameter `dropout_prob`\n",
    "* If `max_pooling` is set to True, then add a MaxPooling2D layer with 2x2 pool size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d548a6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(inputs=None, n_filters=32, dropout_prob=0, max_pooling=True):\n",
    "    \"\"\"\n",
    "    Convolutional downsampling block\n",
    "    \n",
    "    Arguments:\n",
    "        inputs -- Input tensor\n",
    "        n_filters -- Number of filters for the convolutional layers\n",
    "        dropout_prob -- Dropout probability\n",
    "        max_pooling -- Use MaxPooling2D to reduce the spatial dimensions of the output volume\n",
    "    Returns: \n",
    "        next_layer, skip_connection -- Next layer and skip connection outputs\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Convolutional layers\n",
    "    #TODO replace None\n",
    "    conv = Conv2D(None, # Number of filters\n",
    "                  None, # Kernek size\n",
    "                  activation=None,\n",
    "                  padding=None,\n",
    "                  kernel_initializer=None)(inputs)\n",
    "    conv = Conv2D(None,\n",
    "                  kernel_size=None,\n",
    "                  activation=None,\n",
    "                  padding=None,\n",
    "                  kernel_initializer=None)(conv)\n",
    "    \n",
    "    # Optional dropout layer\n",
    "    if dropout_prob > 0.0:\n",
    "        #TODO replace None\n",
    "        conv = None\n",
    "        \n",
    "    # Optional max pooling layer\n",
    "    if max_pooling:\n",
    "        #TODO replace None\n",
    "        next_layer = None\n",
    "    else:\n",
    "        next_layer = conv\n",
    "        \n",
    "    # Output skip connection\n",
    "    skip_connection = conv\n",
    "    \n",
    "    return next_layer, skip_connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaaadbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=(96, 128, 3)\n",
    "n_filters = 32\n",
    "inputs = Input(input_size)\n",
    "cblock1 = conv_block(inputs, n_filters * 1)\n",
    "model1 = tf.keras.Model(inputs=inputs, outputs=cblock1)\n",
    "\n",
    "output1 = [['InputLayer', [(None, 96, 128, 3)], 0],\n",
    "            ['Conv2D', (None, 96, 128, 32), 896, 'same', 'relu', 'HeNormal'],\n",
    "            ['Conv2D', (None, 96, 128, 32), 9248, 'same', 'relu', 'HeNormal'],\n",
    "            ['MaxPooling2D', (None, 48, 64, 32), 0, (2, 2), (2, 2), 'valid']]\n",
    "\n",
    "print('Block 1:')\n",
    "for layer in summary(model1):\n",
    "    print(layer)\n",
    "\n",
    "comparator(summary(model1), output1)\n",
    "\n",
    "inputs = Input(input_size)\n",
    "cblock1 = conv_block(inputs, n_filters * 32, dropout_prob=0.1, max_pooling=True)\n",
    "model2 = tf.keras.Model(inputs=inputs, outputs=cblock1)\n",
    "\n",
    "output2 = [['InputLayer', [(None, 96, 128, 3)], 0],\n",
    "            ['Conv2D', (None, 96, 128, 1024), 28672, 'same', 'relu', 'HeNormal'],\n",
    "            ['Conv2D', (None, 96, 128, 1024), 9438208, 'same', 'relu', 'HeNormal'],\n",
    "            ['Dropout', (None, 96, 128, 1024), 0, 0.1],\n",
    "            ['MaxPooling2D', (None, 48, 64, 1024), 0, (2, 2),  (2, 2), 'valid']]\n",
    "           \n",
    "print('\\nBlock 2:')   \n",
    "for layer in summary(model2):\n",
    "    print(layer)\n",
    "    \n",
    "comparator(summary(model2), output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79ec626",
   "metadata": {},
   "source": [
    "<a name='2.3'></a>\n",
    "## <font color='darkviolet'> 2.3 - Decoder (Upsampling Block)\n",
    "\n",
    "The decoder, or upsampling block, upsamples the features back to the original image size. At each upsampling level, you'll take the output of the corresponding encoder block and concatenate it before feeding to the next decoder block.\n",
    "\n",
    "<!-- <img src=\"data/images/decoder.png\" style=\"width:500px;height:500;\">\n",
    "<caption><center> <u><b>Figure 4</u></b>: The U-Net Decoder up close <br> </center></caption> -->\n",
    "\n",
    "There are two new components in the decoder: `up` and `merge`. These are the transpose convolution and the skip connections. In addition, there are two more convolutional layers set to the same parameters as in the encoder. \n",
    "\n",
    "Here you'll encounter the `Conv2DTranspose` layer, which performs the inverse of the `Conv2D` layer. You can read more about it [here.](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose)\n",
    "\n",
    "\n",
    "<a name='ex-2.3.1'></a>\n",
    "### <font color='blue'> Exercise 2.3.1 - upsampling_block\n",
    "\n",
    "Implement `upsampling_block(...)`.\n",
    "    \n",
    "The function takes the arguments `expansive_input` (the input tensor from the previous layer) and `contractive_input` (the input tensor from the previous skip layer). The number of filters is the same as in the downsampling block.\n",
    "\n",
    "A Conv2DTranspose layer is applied to `expansive_input` with n_filters, a shape of (3,3), a stride of (2,2), and padding set to `same`. The output from this layer is concatenated to `contractive_input` to create skip connections.\n",
    "\n",
    "For the final component, set the parameters for two Conv2D layers to the same values that you set for the two Conv2D layers in the encoder (ReLU activation, He normal initializer, `same` padding). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4875e20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsampling_block(expansive_input, contractive_input, n_filters=32):\n",
    "    \"\"\"\n",
    "    Convolutional upsampling block\n",
    "    \n",
    "    Arguments:\n",
    "        expansive_input -- Input tensor from previous layer\n",
    "        contractive_input -- Input tensor from previous skip layer\n",
    "        n_filters -- Number of filters for the convolutional layers\n",
    "    Returns: \n",
    "        conv -- Tensor output\n",
    "    \"\"\"    \n",
    "    \n",
    "    # Upsample the expansive input using a transposed convolution\n",
    "    #TODO replace None\n",
    "    up = Conv2DTranspose(None, # Number of filters\n",
    "                 None, # Kernek size\n",
    "                 strides=None,            # Double the spatial dimensions\n",
    "                 padding=None)(expansive_input)\n",
    "    \n",
    "    # Merge the upsampled output and the contractive input\n",
    "    merge = concatenate([up, contractive_input], axis=3)\n",
    "    \n",
    "    # Apply two 3x3 convolutions with ReLU activation, \"same\" padding and He normal initialization\n",
    "    #TODO replace None\n",
    "    conv = Conv2D(None, # Number of filters \n",
    "                 None, # Kernek size\n",
    "                 activation=None,\n",
    "                 padding=None,\n",
    "                 kernel_initializer=None)(merge)\n",
    "    conv = Conv2D(None,  # Number of filters \n",
    "                 None,   # Kernel size\n",
    "                 activation=None,\n",
    "                 padding=None,\n",
    "                 kernel_initializer=None)(conv)\n",
    "       \n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eb5af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size1=(12, 16, 256)\n",
    "input_size2 = (24, 32, 128)\n",
    "n_filters = 32\n",
    "expansive_inputs = Input(input_size1)\n",
    "contractive_inputs =  Input(input_size2)\n",
    "cblock1 = upsampling_block(expansive_inputs, contractive_inputs, n_filters * 1)\n",
    "model1 = tf.keras.Model(inputs=[expansive_inputs, contractive_inputs], outputs=cblock1)\n",
    "\n",
    "output1 = [['InputLayer', [(None, 12, 16, 256)], 0],\n",
    "            ['Conv2DTranspose', (None, 24, 32, 32), 73760],\n",
    "            ['InputLayer', [(None, 24, 32, 128)], 0],\n",
    "            ['Concatenate', (None, 24, 32, 160), 0],\n",
    "            ['Conv2D', (None, 24, 32, 32), 46112, 'same', 'relu', 'HeNormal'],\n",
    "            ['Conv2D', (None, 24, 32, 32), 9248, 'same', 'relu', 'HeNormal']]\n",
    "\n",
    "print('Block 1:')\n",
    "for layer in summary(model1):\n",
    "    print(layer)\n",
    "\n",
    "comparator(summary(model1), output1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4d177b",
   "metadata": {},
   "source": [
    "<a name='2.4'></a>\n",
    "## <font color='darkviolet'> 2.4 - Build the Model\n",
    "\n",
    "You will combine the **encoder** and **decoder** to complete the architecture. You need to specify the number of output channels, which is 23 in this case. This is because there are 23 different labels for each pixel in the self-driving car dataset.\n",
    "\n",
    "<a name='ex-2.4.1'></a>\n",
    "### <font color='blue'> Exercise 2.4.1 - unet_model\n",
    "\n",
    "For the function `unet_model`, specify the input shape, number of filters, and number of classes (23 in this case).\n",
    "\n",
    "For the first half of the model:\n",
    "\n",
    "* Begin with a conv block that takes as inputs the model and the number of filters\n",
    "* Then, chain the first output element of each block to the input of the next convolutional block\n",
    "* Next, double the number of filters at each step\n",
    "* Beginning with `conv_block4`, add `dropout_prob` of 0.3\n",
    "* For the final conv_block, set `dropout_prob` to 0.3 again, and turn off max pooling  \n",
    "\n",
    "For the second half:\n",
    "\n",
    "* Use cblock5 as expansive_input and cblock4 as contractive_input, with `n_filters` * 8. This is your bottleneck layer\n",
    "* Chain the output of the previous block as expansive_input and the corresponding contractive block output\n",
    "* Note that you must use the second element of the contractive block before the max pooling layer \n",
    "* At each step, use half the number of filters of the previous block\n",
    "* `conv9` is a Conv2D layer with ReLU activation, He normal initializer, `same` padding\n",
    "* Finally, `conv10` is a Conv2D that takes the number of classes as the filter, a kernel size of 1, and `same` padding. The output of `conv10` is the output of your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ea78d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_model(input_size=(96, 128, 3), n_filters=32, n_classes=23):\n",
    "    \"\"\"\n",
    "    Unet model\n",
    "    \n",
    "    Arguments:\n",
    "        input_size -- Input shape \n",
    "        n_filters -- Number of filters for the convolutional layers\n",
    "        n_classes -- Number of output classes\n",
    "    Returns: \n",
    "        model -- tf.keras.Model\n",
    "    \"\"\"\n",
    "    inputs = Input(input_size)\n",
    "    \n",
    "    #=== Contracting Path (encoding)\n",
    "    #TODO replace None\n",
    "    # Add a conv_block with the inputs of the unet_ model and n_filters\n",
    "    cblock1 = conv_block(None, None)\n",
    "    \n",
    "    # Chain the first element of the output of each block to be the input of the next conv_block. \n",
    "    # Double the number of filters at each new step\n",
    "    cblock2 = conv_block(None, None)\n",
    "    cblock3 = conv_block(None, None)\n",
    "    cblock4 = conv_block(None, None, dropout_prob=None) # Include a dropout_prob of 0.3 for this layer\n",
    "    \n",
    "    # Include a dropout_prob of 0.3 for this layer, and avoid the max_pooling layer\n",
    "    cblock5 = conv_block(None, None, dropout_prob=None, max_pooling=None) \n",
    "    \n",
    "    #=== Expanding Path (decoding)\n",
    "    #TODO replace None\n",
    "    # Add the first upsampling_block.\n",
    "    # Use the cblock5[0] as expansive_input and cblock4[1] as contractive_input and n_filters * 8\n",
    "    ublock6 = upsampling_block(None, None, None)\n",
    "    \n",
    "    # Chain the output of the previous block as expansive_input and the corresponding contractive block output.\n",
    "    # Note that you must use the second element of the contractive block i.e before the maxpooling layer. \n",
    "    # At each step, use half the number of filters of the previous block \n",
    "    ublock7 = upsampling_block(None, None, None)\n",
    "    ublock8 = upsampling_block(None, None, None)\n",
    "    ublock9 = upsampling_block(None, None, None)\n",
    "    \n",
    "    conv9 = Conv2D(n_filters,\n",
    "                   3,\n",
    "                   activation='relu',\n",
    "                   padding='same',\n",
    "                   kernel_initializer='he_normal')(ublock9)\n",
    "    \n",
    "    # Add a Conv2D layer with n_classes filter, kernel size of 1 and a 'same' padding\n",
    "    conv10 = Conv2D(None, None, padding=None)(conv9)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=conv10)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87619dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import data.outputs\n",
    "img_height = 96\n",
    "img_width = 128\n",
    "num_channels = 3\n",
    "\n",
    "unet = unet_model((img_height, img_width, num_channels))\n",
    "comparator(summary(unet), data.outputs.unet_model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d9f381",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 96\n",
    "img_width = 128\n",
    "num_channels = 3\n",
    "\n",
    "unet = unet_model((img_height, img_width, num_channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b159171",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d73910a",
   "metadata": {},
   "source": [
    "<a name='2.5'></a>\n",
    "## <font color='darkviolet'> 2.5 - Loss Function\n",
    "\n",
    "In semantic segmentation, you have one mask per object class, where each pixel in the mask has a probability that it belongs to a certain class. The highest probability indicates the correct class. \n",
    "\n",
    "Use **sparse categorical crossentropy** as the loss function for pixel-wise multiclass prediction, which is more efficient than other loss functions for many classes https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be3d1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d15849",
   "metadata": {},
   "source": [
    "Define a function that allows you to display both an input image, and its ground truth, i.e., the true mask. The true mask is what your trained model output is aiming to get as close to as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1279d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(display_list):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7265b2c3",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## <font color='darkviolet'> 3 - Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822a1cbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "# VAL_SUBSPLITS = 5\n",
    "BUFFER_SIZE = 500\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = processed_image_ds.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "print(processed_image_ds.element_spec)\n",
    "model_history = unet.fit(train_dataset, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128bd7b1",
   "metadata": {},
   "source": [
    "#### <font color='blue'> Q3.1 Describe your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3ef32d",
   "metadata": {},
   "source": [
    "<a name='3-1'></a>\n",
    "### <font color='darkviolet'> 3.1 - Create Predicted Masks \n",
    "\n",
    "Now, define a function that uses `tf.argmax` in the axis of the number of classes to return the index with the largest value and merge the prediction into a single image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f71cd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(pred_mask):\n",
    "    pred_mask = tf.argmax(pred_mask, axis=-1)\n",
    "    pred_mask = pred_mask[..., tf.newaxis]\n",
    "    return pred_mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6664b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model_history.history[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a636d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(dataset=None, num=1):\n",
    "    \"\"\"\n",
    "    Displays the first image of each of the num batches\n",
    "    \"\"\"\n",
    "    if dataset:\n",
    "        for image, mask in dataset.take(num):\n",
    "            pred_mask = unet.predict(image)\n",
    "            display([image[0], mask[0], create_mask(pred_mask)])\n",
    "    else:\n",
    "        display([sample_image, sample_mask,\n",
    "             create_mask(unet.predict(sample_image[tf.newaxis, ...]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2d9ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions(train_dataset, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9489aae3",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "### <font color='darkviolet'> 4 - Conclusion\n",
    "\n",
    "#### <font color='blue'> Q4.1 What is semantic image segmentation?\n",
    "#### <font color='blue'> Q4.2 How does U-Net perform downsampling and upsampling?\n",
    "#### <font color='blue'> Q4.3 What are skip connections and why are they used in U-Net?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
