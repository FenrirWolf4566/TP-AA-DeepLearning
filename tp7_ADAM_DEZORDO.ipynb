{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "110b3ea8",
   "metadata": {},
   "source": [
    "<font  style=\"font-size: 4rem; color: darkviolet\"> Convolutional Neural Networks in TensorFlow </font>\n",
    "\n",
    "AA - 2022/23 - TP7\n",
    "\n",
    "*This assignement is inspired by the Deep Learning course on Coursera by Andrew Ng, Stanford University, for which we are thankful.*\n",
    "\n",
    "In this assignment, you will learn how to build and train Convolutional Neural Networks (ConvNets) using TensorFlow Keras API. You will be working on two different tasks. First, you will build a binary classifier using the Sequential API to classify the mood of a given person as either positive or negative. Next, you will build a multiclass classifier using the Functional API to identify sign language digits.\n",
    "\n",
    "Before starting this assignment, you should have a basic understanding of TensorFlow.\n",
    "\n",
    "You're encouraged to read the official documentation. You can find the docs for the Sequential and Functional APIs here:\n",
    "\n",
    "https://www.tensorflow.org/guide/keras/sequential_model\n",
    "\n",
    "https://www.tensorflow.org/guide/keras/functional\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "- [1 - The Sequential API](#1)\n",
    "    - [1.1 - The Happy House Dataset](#1.1)\n",
    "    - [1.2 - Create the Sequential Model](#1.2)\n",
    "        - [Exercise 1.2.1 - happyModel](#ex-1.2.1)\n",
    "    - [1.3 - Train and Evaluate the Model](#1.3)\n",
    "- [2 - The Functional API](#2)\n",
    "    - [2.1 - The SIGNS Dataset](#2.1)\n",
    "    - [2.2 - Forward Propagation](#2.2)\n",
    "        - [Exercise 2.2.1 - convolutional_model](#ex-2.2.1)\n",
    "    - [2.3 - Train the Model](#2.3)\n",
    "    - [2.4 - History](#2.4)\n",
    "- [3 - Exercise - output volume](#ex-3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7de70dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 07:29:54.773928: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-12 07:29:54.913064: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-12 07:29:54.913085: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-04-12 07:29:55.685202: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-12 07:29:55.685289: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-12 07:29:55.685298: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imread\n",
    "import scipy\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfl\n",
    "from tensorflow.python.framework import ops\n",
    "from data.cnn_utils import *\n",
    "from data.test_utils import summary, comparator\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51916b6",
   "metadata": {},
   "source": [
    "# <a name='1'></a>\n",
    "# <font color='darkviolet'> 1 - The Sequential API\n",
    "\n",
    "When it comes to practical applications of deep learning, programming frameworks are essential tools that offer a wide range of built-in functions to help you create and train models with ease. One such framework is Keras, which is a high-level abstraction built on top of TensorFlow that provides even more simplified and optimized model building and training.\n",
    "\n",
    "You'll be using Keras' Sequential API to create a deep learning model. This API allows you to build models layer by layer, and is particularly well-suited for simple tasks where each layer has exactly one input tensor and one output tensor. Using the Sequential API is straightforward and easy to understand, making it a great choice for beginners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc9235a",
   "metadata": {},
   "source": [
    "<a name='1.1'></a>\n",
    "## <font color='darkviolet'> 1.1 - The Happy House dataset\n",
    "\n",
    "The Happy House dataset comprises images of individuals' faces, and your objective will be to create a ConvNet that can accurately identify whether these individuals are smiling or not. This is an important task because only those who are smiling will be granted entry into the Happy House."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd1efa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_happy_dataset():\n",
    "    # Load the training dataset\n",
    "    train_dataset = h5py.File('data/train_happy.h5', \"r\")\n",
    "    \n",
    "    # Extract the features and labels for the training set\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # labels\n",
    "\n",
    "    # Load the test dataset\n",
    "    test_dataset = h5py.File('data/test_happy.h5', \"r\")\n",
    "    \n",
    "    # Extract the features and labels for the test set\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # labels\n",
    "\n",
    "    # Extract the list of classes\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) \n",
    "    \n",
    "    # Reshape the labels to match the expected shape\n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    # Return the dataset as a tuple\n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a7242c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Happy House dataset\n",
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_happy_dataset()\n",
    "\n",
    "# Normalize the image vectors by dividing by 255\n",
    "X_train = X_train_orig / 255.\n",
    "X_test = X_test_orig / 255.\n",
    "\n",
    "# Reshape the labels to match the expected shape (m, 1), where m is the number of examples\n",
    "Y_train = Y_train_orig.T\n",
    "Y_test = Y_test_orig.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faa87ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples = 600\n",
      "Number of test examples = 150\n",
      "X_train shape: (600, 64, 64, 3)\n",
      "Y_train shape: (600, 1)\n",
      "X_test shape: (150, 64, 64, 3)\n",
      "Y_test shape: (150, 1)\n"
     ]
    }
   ],
   "source": [
    "# Print the shapes of the datasets and labels\n",
    "print(\"Number of training examples = \" + str(X_train.shape[0]))\n",
    "print(\"Number of test examples = \" + str(X_test.shape[0]))\n",
    "print(\"X_train shape: \" + str(X_train.shape))\n",
    "print(\"Y_train shape: \" + str(Y_train.shape))\n",
    "print(\"X_test shape: \" + str(X_test.shape))\n",
    "print(\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebf54c8",
   "metadata": {},
   "source": [
    "#### <font color='blue'> Q1.1 Describe your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e46a985",
   "metadata": {},
   "source": [
    "<a name='1.2'></a>\n",
    "## <font color='darkviolet'> 1.2 - Create the Sequential Model \n",
    "    \n",
    "The **TensorFlow Keras Sequential** API is useful for building simple models where layers are arranged in a sequential order. You can add and remove layers using the `.add()` and `.pop()` methods, respectively. A Sequential model can be thought of as a list of layers, where the order in which they are specified matters. \n",
    "\n",
    "In Keras, you must specify the input shape in advance for any layer construction, since the shape of the weights is based on the shape of the inputs. To create a Sequential model, pass a list of layers to the Sequential constructor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8837b35",
   "metadata": {},
   "source": [
    "<a name='ex-1.2.1'></a>\n",
    "### <font color='blue'> Exercise 1.2.1 - happyModel\n",
    "\n",
    "Implement the `happyModel` function below to build the following model: `ZEROPAD2D -> CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> FLATTEN -> DENSE`. \n",
    "    \n",
    "Take help from [tf.keras.layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers).\n",
    "\n",
    "Plug in the following parameters for all the steps:\n",
    "\n",
    " - [ZeroPadding2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ZeroPadding2D): padding 3, input shape 64 x 64 x 3\n",
    " - [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D): Use 32 7x7 filters, stride 1\n",
    " - [BatchNormalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization): for axis 3\n",
    " - [ReLU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU)\n",
    " - [MaxPool2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D): Using default parameters\n",
    " - [Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten) the previous output.\n",
    " - Fully-connected ([Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)) layer: Apply a fully connected layer with 1 neuron and a sigmoid activation. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d548a6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def happyModel():\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the binary classification model:\n",
    "    ZEROPAD2D -> CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> FLATTEN -> DENSE\n",
    "    \n",
    "    Arguments:\n",
    "    None\n",
    "\n",
    "    Returns:\n",
    "    model -- TF Keras model (object containing the information for the entire training process) \n",
    "    \"\"\"\n",
    "\n",
    "    # Define the layers of the model using the Sequential API\n",
    "    model = tf.keras.Sequential([\n",
    "        \n",
    "        # ZeroPadding2D with padding 3, input shape of 64 x 64 x 3\n",
    "        #TODO\n",
    "\n",
    "        \n",
    "        # Conv2D with 32 7x7 filters and stride of 1\n",
    "        #TODO\n",
    "\n",
    "        \n",
    "        # BatchNormalization for axis 3\n",
    "        #TODO\n",
    "\n",
    "        \n",
    "        # ReLU\n",
    "        #TODO\n",
    "\n",
    "        \n",
    "        # Max Pooling 2D with default parameters\n",
    "        #TODO\n",
    "\n",
    "        \n",
    "        # Flatten layer\n",
    "        #TODO\n",
    "\n",
    "        \n",
    "        # Dense layer with 1 unit & 'sigmoid' activation\n",
    "        #TODO\n",
    "\n",
    "    ])\n",
    "    \n",
    "    # Return the model\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eaaadbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mAll tests passed!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 07:29:56.584711: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-04-12 07:29:56.584777: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-04-12 07:29:56.584828: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (fgalassi): /proc/driver/nvidia/version does not exist\n",
      "2023-04-12 07:29:56.585259: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "happy_model = happyModel()\n",
    "# Print a summary for each layer\n",
    "for layer in summary(happy_model):\n",
    "    print(layer)\n",
    "    \n",
    "output = [['ZeroPadding2D', (None, 70, 70, 3), 0, ((3, 3), (3, 3))],\n",
    "            ['Conv2D', (None, 64, 64, 32), 4736, 'valid', 'linear', 'GlorotUniform'],\n",
    "            ['BatchNormalization', (None, 64, 64, 32), 128],\n",
    "            ['ReLU', (None, 64, 64, 32), 0],\n",
    "            ['MaxPooling2D', (None, 32, 32, 32), 0, (2, 2), (2, 2), 'valid'],\n",
    "            ['Flatten', (None, 32768), 0],\n",
    "            ['Dense', (None, 1), 32769, 'sigmoid']]\n",
    "    \n",
    "comparator(summary(happy_model), output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79ec626",
   "metadata": {},
   "source": [
    "After creating a deep learning model, the next step is to compile it for training by specifying an optimizer and loss function. In addition, you can also specify metrics such as accuracy to track during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4875e20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_model.compile(optimizer='adam',\n",
    "                   loss='binary_crossentropy',\n",
    "                   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84757ccd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mhappy_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ESIR_data_mining/AA_3.9_venv/lib/python3.9/site-packages/keras/engine/training.py:3292\u001b[0m, in \u001b[0;36mModel.summary\u001b[0;34m(self, line_length, positions, print_fn, expand_nested, show_trainable, layer_range)\u001b[0m\n\u001b[1;32m   3262\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prints a string summary of the network.\u001b[39;00m\n\u001b[1;32m   3263\u001b[0m \n\u001b[1;32m   3264\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3289\u001b[0m \u001b[38;5;124;03m    ValueError: if `summary()` is called before the model is built.\u001b[39;00m\n\u001b[1;32m   3290\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[0;32m-> 3292\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3293\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model has not yet been built. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3294\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuild the model first by calling `build()` or by calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3295\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe model on a batch of data.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3296\u001b[0m     )\n\u001b[1;32m   3297\u001b[0m layer_utils\u001b[38;5;241m.\u001b[39mprint_summary(\n\u001b[1;32m   3298\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3299\u001b[0m     line_length\u001b[38;5;241m=\u001b[39mline_length,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3304\u001b[0m     layer_range\u001b[38;5;241m=\u001b[39mlayer_range,\n\u001b[1;32m   3305\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data."
     ]
    }
   ],
   "source": [
    "happy_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c164ebc",
   "metadata": {},
   "source": [
    "#### <font color='blue'> Q1.2.1 What is the difference between trainable and non-trainable parameters in a deep learning model?\n",
    "\n",
    "#### <font color='blue'> Q1.2.2 How to compute the number of parameters reported per layer? Can you identify the origin of the 64 non-trainable parameters?\n",
    "        \n",
    "#### <font color='blue'> Q1.2.3 You can note that the first output dimension of a layers is *None*. Can you explain why this is the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4d177b",
   "metadata": {},
   "source": [
    "<a name='1.3'></a>\n",
    "## <font color='darkviolet'> 1.3 - Train and Evaluate the Model\n",
    "\n",
    "Once you've created and compiled your model, and verified its structure using `model.summary()`, it's time to start training. Simply call `.fit()` to begin the training process.\n",
    "\n",
    "One of the advantages of using TensorFlow is that it automates many of the tedious and complex aspects of machine learning, such as backpropagation. You do have the option to specify the number of epochs and the size of the mini-batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eb9286",
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_model.fit(X_train, Y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631f6ad8",
   "metadata": {},
   "source": [
    "#### <font color='blue'> Q1.3.1 What does each output line indicate ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c91d46b",
   "metadata": {},
   "source": [
    "After training your model using `.fit()`, you can evaluate its performance on your test set by calling `.evaluate()`. This function returns the value of the **loss function** and any **performance metrics** you specified during compilation. In this case, the loss function used was binary_crossentropy and the metric used was accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf0327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bda53e",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "# <font color='darkviolet'> 2 - The Functional API\n",
    "\n",
    "The [Functional API](https://www.tensorflow.org/guide/keras/functional) in TensorFlow can handle more complex models than the Sequential API, allowing for shared layers, and layers with multiple inputs or outputs. \n",
    "One example of a more complex layer connection is a skip connection, which skips some layers in the network and feeds the output directly to a later layer in the network.\n",
    "    \n",
    "You will use Keras flexible Functional API to build a ConvNet that can differentiate between 6 sign language digits. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba878913",
   "metadata": {},
   "source": [
    "<a name='2.1'></a>\n",
    "## <font color='darkviolet'> 2.1 - The SIGNS Dataset\n",
    "\n",
    "The SIGNS dataset is a collection of 6 signs representing numbers from 0 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1661675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_signs_dataset():\n",
    "    # Load the training dataset\n",
    "    train_dataset = h5py.File('data/train_signs.h5', \"r\")\n",
    "    \n",
    "    # Extract the features and labels for the training set\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # labels\n",
    "\n",
    "    # Load the test dataset\n",
    "    test_dataset = h5py.File('data/test_signs.h5', \"r\")\n",
    "    \n",
    "    # Extract the features and labels for the test set\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # labels\n",
    "\n",
    "    # Extract the list of classes\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) \n",
    "    \n",
    "    # Reshape the labels to match the expected shape\n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    # Return the dataset as a tuple\n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ab9022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data (signs)\n",
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_signs_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28451ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of an image from the dataset\n",
    "index = 9\n",
    "plt.imshow(X_train_orig[index])\n",
    "print (\"y = \" + str(np.squeeze(Y_train_orig[:, index])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb0ff82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the pixel values of the images by dividing each pixel value by 255\n",
    "X_train = X_train_orig/255.\n",
    "X_test = X_test_orig/255.\n",
    "\n",
    "# Convert the target labels to one-hot vectors\n",
    "Y_train = convert_to_one_hot(Y_train_orig, 6).T\n",
    "Y_test = convert_to_one_hot(Y_test_orig, 6).T\n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159322ff",
   "metadata": {},
   "source": [
    "#### <font color='blue'> Q2.1.1 Describe your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780fa06a",
   "metadata": {},
   "source": [
    "<a name='2.2'></a>\n",
    "### <font color='darkviolet'> 2.2 - Forward Propagation\n",
    "\n",
    "In TensorFlow, there are built-in functions that implement the convolution steps. By now, you should be familiar with how TensorFlow builds computational graphs. In the [Functional API](https://www.tensorflow.org/guide/keras/functional), you create a graph of layers. This is what allows such great flexibility.\n",
    "\n",
    "However, the following model could also be defined using the Sequential API since the information flow is on a single line. Anyhw, we want you to learn to use the Functional API.\n",
    "\n",
    "Begin building your graph of layers by creating an input node as a callable object:\n",
    "\n",
    "- **input_img = tf.keras.Input(shape=input_shape):** \n",
    "\n",
    "Then, create a new node in the graph of layers by calling a layer on the `input_img` object: \n",
    "\n",
    "- **tf.keras.layers.Conv2D(filters= ... , kernel_size= ... , padding='same')(input_img):** Read the full documentation on [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D).\n",
    "\n",
    "- **tf.keras.layers.MaxPool2D(pool_size=(f, f), strides=(s, s), padding='same'):** `MaxPool2D()` downsamples your input using a window of size (f, f) and strides of size (s, s) to carry out max pooling over each window.  For max pooling, you usually operate on a single example at a time and a single channel at a time. Read the full documentation on [MaxPool2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D).\n",
    "\n",
    "- **tf.keras.layers.ReLU():** computes the element-wise ReLU of Z (which can be any shape). You can read the full documentation on [ReLU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU).\n",
    "\n",
    "- **tf.keras.layers.Flatten()**: given a tensor \"P\", this function takes each training (or test) example in the batch and flattens it into a 1D vector.  \n",
    "\n",
    "    * If a tensor P has the shape (batch_size,h,w,c), it returns a flattened tensor with shape (batch_size, k), where $k=h \\times w \\times c$.  \"k\" equals the product of all the dimension sizes other than the first dimension.\n",
    "    \n",
    "    * For example, given a tensor with dimensions [100, 2, 3, 4], it flattens the tensor to be of shape [100, 24], where 24 = 2 * 3 * 4.  You can read the full documentation on [Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten).\n",
    "\n",
    "- **tf.keras.layers.Dense(units= ... , activation='softmax')(F):** given the flattened input F, it returns the output computed using a fully connected layer. You can read the full documentation on [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense).\n",
    "\n",
    "In the last function above (`tf.keras.layers.Dense()`), the fully connected layer automatically initializes weights in the graph and keeps on training them as you train the model. Hence, you did not need to initialize those weights when initializing the parameters.\n",
    "\n",
    "Lastly, before creating the model, you'll need to define the output using the last of the function's compositions (in this example, a Dense layer): \n",
    "\n",
    "- **outputs = tf.keras.layers.Dense(units=6, activation='softmax')(F)** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2a2305",
   "metadata": {},
   "source": [
    "<a name='ex-2.2.1'></a>\n",
    "### <font color='blue'> Exercise 2.2.1 - convolutional_model\n",
    "\n",
    "Implement the `convolutional_model` function below to build the following model: `CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> DENSE`. Use the functions above! \n",
    "\n",
    "Also, plug in the following parameters for all the steps:\n",
    "\n",
    " - [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D): Use 8 filters 4 by 4 , stride 1, padding is \"SAME\"\n",
    " - [ReLU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU)\n",
    " - [MaxPool2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D): Use an 8 by 8 filter size and an 8 by 8 stride, padding is \"SAME\"\n",
    " - **Conv2D**: Use 16 filters 2 by 2, stride 1, padding is \"SAME\"\n",
    " - **ReLU**\n",
    " - **MaxPool2D**: Use a 4 by 4 filter size and a 4 by 4 stride, padding is \"SAME\"\n",
    " - [Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten) the previous output.\n",
    " - Fully-connected ([Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)) layer: apply a fully connected layer with 6 neurons and a softmax activation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c9192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_model(input_shape):\n",
    "    \"\"\"\n",
    "    Implements a convolutional neural network model using Keras.\n",
    "\n",
    "    Arguments:\n",
    "    input_shape -- shape of the input data\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input layer\n",
    "    input_img = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    # CONV2D: 8 filters 4x4, stride of 1, padding 'SAME'\n",
    "    #TODO\n",
    "    \n",
    "    # Apply ReLU activation function\n",
    "    #TODO\n",
    "    \n",
    "    # MAXPOOL: window 8x8, stride 8, padding 'SAME'\n",
    "    #TODO\n",
    "    \n",
    "    # CONV2D: 16 filters 2x2, stride 1, padding 'SAME'\n",
    "    #TODO\n",
    "    \n",
    "    # Apply ReLU activation function\n",
    "    #TODO\n",
    "    \n",
    "    # MAXPOOL: window 4x4, stride 4, padding 'SAME'\n",
    "    #TODO\n",
    "    \n",
    "    # Flatten\n",
    "    #TODO\n",
    "    \n",
    "    # Dense layer\n",
    "    # 6 neurons in output layer. Hint: one of the arguments should be \"activation='softmax'\" \n",
    "    #TODO\n",
    "    \n",
    "    # Define the Keras model\n",
    "    model = tf.keras.Model(inputs=input_img, outputs=out)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9c7088",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model = convolutional_model((64, 64, 3))\n",
    "conv_model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "conv_model.summary()\n",
    "    \n",
    "output = [['InputLayer', [(None, 64, 64, 3)], 0],\n",
    "        ['Conv2D', (None, 64, 64, 8), 392, 'same', 'linear', 'GlorotUniform'],\n",
    "        ['ReLU', (None, 64, 64, 8), 0],\n",
    "        ['MaxPooling2D', (None, 8, 8, 8), 0, (8, 8), (8, 8), 'same'],\n",
    "        ['Conv2D', (None, 8, 8, 16), 528, 'same', 'linear', 'GlorotUniform'],\n",
    "        ['ReLU', (None, 8, 8, 16), 0],\n",
    "        ['MaxPooling2D', (None, 2, 2, 16), 0, (4, 4), (4, 4), 'same'],\n",
    "        ['Flatten', (None, 64), 0],\n",
    "        ['Dense', (None, 6), 390, 'softmax']]\n",
    "    \n",
    "comparator(summary(conv_model), output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8572c47",
   "metadata": {},
   "source": [
    "Both the Sequential and Functional APIs return a TF Keras model object. The only difference is how inputs are handled inside the object model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c50d86",
   "metadata": {},
   "source": [
    "#### <font color='blue'> Q2.2.1 How to compute the number of parameters reported per layer? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b396aec5",
   "metadata": {},
   "source": [
    "<a name='2.3'></a>\n",
    "### <font color='darkviolet'> 2.3 - Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ea78d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).batch(64)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87619dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the size of the train_dataset\n",
    "train_size = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "print('Number of batches in train_dataset:', train_size)\n",
    "print('Number of samples in train_dataset:', train_size * 64)\n",
    "\n",
    "# Print the size of the test_dataset\n",
    "test_size = tf.data.experimental.cardinality(test_dataset).numpy()\n",
    "print('Number of batches in test_dataset:', test_size)\n",
    "print('Number of samples in test_dataset:', test_size * 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da94b1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the contents of the first batch of the train_dataset\n",
    "for X, Y in train_dataset.take(1):\n",
    "    print('Shape of input features (X):', X.shape)\n",
    "    print('Shape of labels (Y):', Y.shape)\n",
    "    print('Input features (X):', X)\n",
    "    print('Labels (Y):', Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a920fcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "conv_model.compile(optimizer='adam',\n",
    "                   loss='categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = conv_model.fit(train_dataset,\n",
    "                          epochs=100,\n",
    "                          validation_data=test_dataset,\n",
    "                          verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e6e2e1",
   "metadata": {},
   "source": [
    "<a name='2.4'></a>\n",
    "### <font color='darkviolet'> 2.4 - History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c85a3a8",
   "metadata": {},
   "source": [
    "The history object is an output of the `.fit()` operation, and provides a record of all the loss and metric values in memory. It's stored as a dictionary that you can retrieve at `history.history`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aff14af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the loss and accuracy values\n",
    "df_loss_acc = pd.DataFrame(history.history)\n",
    "\n",
    "# Extract the training and validation loss values and rename the columns\n",
    "df_loss = df_loss_acc[['loss', 'val_loss']]\n",
    "df_loss = df_loss.rename(columns={'loss': 'Training Loss', 'val_loss': 'Validation Loss'})\n",
    "\n",
    "# Extract the training and validation accuracy values and rename the columns\n",
    "df_acc = df_loss_acc[['accuracy', 'val_accuracy']]\n",
    "df_acc = df_acc.rename(columns={'accuracy': 'Training Accuracy', 'val_accuracy': 'Validation Accuracy'})\n",
    "\n",
    "# Plot the training and validation loss values\n",
    "df_loss.plot(title='Model Loss', figsize=(12, 8)).set(xlabel='Epoch', ylabel='Loss')\n",
    "\n",
    "# Plot the training and validation accuracy values\n",
    "df_acc.plot(title='Model Accuracy', figsize=(12, 8)).set(xlabel='Epoch', ylabel='Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8382f2",
   "metadata": {},
   "source": [
    "#### <font color='blue'> Q2.4.1 Observe your training history and describe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c8fbf9",
   "metadata": {},
   "source": [
    "<a name='ex-3'></a>\n",
    "### <font color='blue'> Exercise 3 - output volume computation\n",
    "    \n",
    "You have an input volume that is 32 x 32 x 16, and apply max pooling with a stride of 2 and a filter size of 2.\n",
    "    \n",
    "What is the output volume?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
